# IRIS - Issues & Flagged Items

> **Purpose**: Track items needing attention before/during IRIS implementation
> **Generated from**: specs/BLUEPRINT-project-staratlas-20251201.yaml

**Last Updated**: 2025-12-05 (Streaming architecture riff session)

---

## Critical Priority (Riff Session 2025-12-05)

### ARCH-004: STT Latency Investigation - RESOLVED
**Severity**: ‚úÖ Resolved | **Created**: 2025-12-05 | **Resolved**: 2025-12-05
**Component**: voice-backend (faster-whisper)

**Original Problem**: Getting 181ms STT latency on RTX 4090. Should be sub-50ms.

**Investigation Results** (2025-12-05):
The 181ms was **first-run cold start**, not actual transcription time.

**Actual Benchmarks** (RTX 4090, base model, int8, after warmup):

| Audio Duration | beam_size=5 (old) | beam_size=1 (new) |
|----------------|-------------------|-------------------|
| 2s (typical cmd) | 43ms | 22-28ms |
| 6s (long)      | 87ms | 55-58ms |

**Root Cause**: Default `beam_size=5` was unnecessarily slow for voice commands.

**Fix Applied**: Changed default `beam_size` from 5 to 1 in `src/stt.py:85`.
- ~50% latency reduction with minimal accuracy impact for short voice commands
- Now meeting sub-50ms target (22-28ms for typical 2s commands)

**Streaming Architecture**: Not required for current latency targets. Batch mode with
optimized beam_size is sufficient. RealtimeSTT remains available in `src/stt_streaming.py`
if future requirements demand partial transcripts during speech.

**References**:
- `test_streaming_stt.py` - Benchmark script with results
- `specs/DESIGN-adaptive-verbosity.md` - Streaming architecture design (future use)

**Future Optimization (if sub-20ms needed)**:
Custom streaming wrapper for maximum speed:
- Silero VAD for speech detection (~50ms)
- Custom faster-whisper streaming wrapper with 15-20ms chunks
- Forward pass every 100ms for partials
- Lighter weight than RealtimeSTT (no portaudio dependency)
- Theoretical: <15ms transcription latency possible with aggressive buffering
- ~2-4 hours to implement if batch mode latency becomes a bottleneck

---

### ARCH-005: VAD (Voice Activity Detection) Missing
**Severity**: üî¥ Critical | **Created**: 2025-12-05
**Component**: voice-backend

**Problem**: No real-time voice activity detection. Currently using push-to-talk.

**Impact**:
- Can't detect end-of-speech automatically for streaming STT
- Can't handle interruptions gracefully
- Can't enable hands-free operation
- Blocks ARCH-004 (streaming STT needs VAD trigger)

**Recommended Solution**: Silero VAD v5 (~50ms detection)

**Action Items**:
- [ ] Integrate Silero VAD into voice-backend
- [ ] Wire VAD to trigger STT finalization
- [ ] Enable interruption detection
- [ ] Design hands-free mode

---

## High Priority (Riff Session 2025-12-05)

### ARCH-006: Interruption Context Desync
**Severity**: üü° High | **Created**: 2025-12-05 | **Status**: Design Complete
**Component**: agent-core

**Problem**: When user interrupts IRIS mid-response, context doesn't reflect what was actually spoken vs generated.

**Scenario**:
```
IRIS generating: "Three updates. First, Alpha docked. Second, fuel critical. Third, repairs."
User interrupts after: "Second, fuel crit‚Äî"
Context shows: Full response delivered (WRONG)
```

**Designed Solution**: Annotate interruptions, don't truncate.

```typescript
interface InterruptionEvent {
  intendedResponse: string;      // full generated response
  spokenUpTo: string;            // parsed from TTS playhead position
  interruptedAtWord: number;     // word/character position
  userInterruption: string;      // what they said
}
```

**Prompt Injection Pattern**:
> "Note: Your previous response was interrupted by the user after '...fuel crit‚Äî'. They only heard up to that point. Your full intended response was: [X]. Their interruption: [Y]"

**Benefits**:
1. Model retains full knowledge of what it intended to say
2. Model understands social context (being cut off)
3. Model can offer to complete: "Fuel's at 15%. I also had an update about repairs - want it?"
4. Enables natural phrases: "As I was saying...", "To finish that thought..."

**Action Items**:
- [ ] Implement TTS playhead tracking
- [ ] Build InterruptionEvent capture
- [ ] Add prompt injection for interruption context
- [ ] Test with VAD-triggered interrupts

**References**: `specs/DESIGN-adaptive-verbosity.md`

---

### ARCH-007: Streaming Architecture Overhaul
**Severity**: üü° High | **Created**: 2025-12-05
**Component**: Full stack

**Problem**: Current architecture waits for complete inputs before processing. Streaming would reduce perceived latency dramatically.

**Target Architecture**:
```
User speaking ‚Üí STT streaming partials ‚Üí LLM pre-thinking on context
User finishes ‚Üí VAD detects ‚Üí LLM finalizes immediately ‚Üí TTS streams first tokens
```

**Key Changes Needed**:
- Streaming STT (ARCH-004)
- VAD integration (ARCH-005)
- LLM receives partials before user finishes ("pre-thinking")
- TTS starts on first token, not complete response

**Two-Tier Response Strategy**:
- **Local models**: Pure streaming, no ack needed (fast enough)
- **Cloud models**: Fast local ack while waiting for first cloud token

**The "Pre-Thinking" Insight**:
```
Partial: "What's the fuel..."
LLM internally: [probably asking about fleet fuel, prep that context]

Partial: "What's the fuel price..."
LLM internally: [oh, market question, pivot to ATLAS/POLIS data]

VAD: [end of speech]
LLM: [already has direction, generates immediately]
```

**Action Items**:
- [ ] Complete ARCH-004 (streaming STT)
- [ ] Complete ARCH-005 (VAD integration)
- [ ] Implement LLM partial context feeding
- [ ] Implement TTS token streaming
- [ ] Benchmark end-to-end latency

---

## Medium Priority (Riff Session 2025-12-05)

### ISSUE-010: Model-Specific System Prompts Needed
**Severity**: üü† Medium | **Created**: 2025-12-05 | **Status**: ‚úÖ Implemented (basic)
**Component**: voice-backend (iris_local.py)

**Problem**: Different models respond better to different prompt structures. Currently using one-size-fits-all.

**Solution Implemented** (2025-12-05):
```python
SYSTEM_PROMPT_BASE = "..."  # Core personality (all models)
MODEL_PROMPTS = {
    "qwen": "...",    # English only, no Chinese/emoji
    "mistral": "...", # More concise
    "llama": "...",   # Stay focused
    "phi": "...",     # Extremely brief
}
get_system_prompt(model_name) ‚Üí combines base + model-specific
```

**Location**: `packages/voice-backend/iris_local.py:76-154`

**Action Items**:
- [x] Create model-specific variants
- [x] Implement dynamic prompt loader (`get_system_prompt()`)
- [x] Logging shows which variant is used
- [ ] A/B test verbosity control per model (future)
- [ ] Extend to narrator module (future)

---

### ISSUE-011: Adaptive Verbosity Control
**Severity**: üü† Medium | **Created**: 2025-12-05 | **Status**: Design Phase
**Component**: agent-core

**Problem**: Response length doesn't adapt to user's communication style.

**Designed Solution**: Three-layer verbosity control:

1. **Style Layer** (system prompt) - baseline ceiling
2. **Explicit Commands** - "quick version" / "all details" (highest priority)
3. **Implicit Mirroring** - infer from user message length, cadence

**Signals**:
- Terse user (one-word commands) ‚Üí short responses
- Verbose user (rambling) ‚Üí acknowledge first, then detail
- Explicit override always wins

**Key Insight - The Affirmation Pattern**:
When users ramble, they often want to be *heard* first:
1. Brief acknowledgment ("Got it - you're dealing with X, Y, Z")
2. Then detailed response

**Action Items**:
- [ ] Implement word count / message length heuristics
- [ ] Detect explicit verbosity keywords
- [ ] Build rolling window for user style detection (last 3-5 messages)
- [ ] Integrate with narrator verbosity levels

**References**: `specs/DESIGN-adaptive-verbosity.md`

---

## Future Architecture (Investigation Items)

### ARCH-008: Subagent Delegation & Multi-Model Infrastructure
**Severity**: üîÆ Future | **Created**: 2025-12-05
**Component**: LLM infrastructure

**Context**: IRIS will need to delegate complex tasks to subagents while preserving main conversation context (similar to Claude Code's task runners).

**Current State (Ollama)**:
- `OLLAMA_MAX_LOADED_MODELS=2` - max 2 models hot
- Model swaps: ~2-3 seconds
- Acceptable for MVP with UX feedback ("Let me look into that...")

**Triggers to Investigate vLLM/SGLang**:
| Trigger | Platform to Evaluate |
|---------|---------------------|
| Need 3+ models simultaneously | vLLM |
| Tool calling unreliable with complex schemas | SGLang |
| Scaling to multiple users | vLLM or TGI |
| Need constrained JSON output | SGLang |

**Subagent Use Cases**:
1. **Task Runners**: Small/fast model for simple tool execution
2. **Deep Thinkers**: Larger model for complex reasoning
3. **Specialists**: Domain-specific fine-tuned models

**MVP Approach**:
- 2 models: conversational (qwen2.5:7b) + thinking (can swap to larger)
- UX communicates delegation: "I need to think about that..."
- User expects delay for complex tasks

**Future Architecture** (when Ollama limits hit):
```
vLLM Server (keeps multiple models warm):
‚îú‚îÄ‚îÄ Main conversational model (always hot)
‚îú‚îÄ‚îÄ Task runner model (hot)
‚îú‚îÄ‚îÄ Deep thinking model (load on demand)
‚îî‚îÄ‚îÄ Specialist models (pool)
```

**Research Needed**:
- [ ] Benchmark Ollama model swap latency
- [ ] Test vLLM concurrent model loading
- [ ] Evaluate SGLang for tool calling reliability
- [ ] Design delegation protocol (context passing)

**Status**: üîÆ Future (implement delegation logic with Ollama first, migrate infrastructure when limits hit)

---

### ARCH-009: Context Window Optimization
**Severity**: üîÆ Future | **Created**: 2025-12-05
**Component**: Conversation management

**Context**: More context = more natural conversation + more useful agent. Need strategies to maximize effective context usage.

**Current State**:
- Rolling 10-turn history (~20 messages)
- No summarization
- Tool results stored in full
- Interruption context persisted

**Problem Areas**:
1. Long conversations lose early context
2. Tool results consume tokens (especially large API responses)
3. No distinction between "must remember" vs "nice to have"

**Optimization Strategies to Investigate**:

| Strategy | Description | Complexity |
|----------|-------------|------------|
| **Conversation summarization** | Compress old turns to summaries | Medium |
| **Sliding window + key facts** | Keep recent + extract important facts | Medium |
| **Tool result compression** | Summarize large API responses | Low |
| **Semantic chunking** | Store/retrieve by relevance | High |
| **Memory tier separation** | Hot (recent) vs warm (session) vs cold (persistent) | High |

**Conversation Summarization Pattern**:
```python
# When context exceeds threshold
if token_count > MAX_CONTEXT * 0.8:
    old_turns = conversation[:-5]  # Keep last 5 fresh
    summary = summarize_llm(old_turns)  # Compress to ~200 tokens
    conversation = [{"role": "system", "content": f"Earlier: {summary}"}] + conversation[-5:]
```

**Tool Result Compression Pattern**:
```python
# For large tool responses
if len(tool_result) > 1000:
    compressed = summarize_llm(f"Summarize key data: {tool_result}")
    return f"[Tool returned {len(tool_result)} chars, summary: {compressed}]"
```

**Key Insight**: Context window is precious for voice assistants. Every token spent on old/redundant info is a token not available for nuanced responses.

**Benchmarks Needed**:
- [ ] Measure current token usage per conversation turn
- [ ] Test summarization quality at various compression ratios
- [ ] Measure latency impact of summarization step
- [ ] A/B test user perception of context-aware vs context-limited

**Status**: üîÆ Future (focus after tools are working)

---

## Low Priority / Future (Riff Session 2025-12-05)

### ISSUE-015: Remove Temporary System Prompt Limitations
**Severity**: üü¢ Low | **Created**: 2025-12-05 | **Status**: Blocked (waiting for tools)
**Component**: voice-backend (iris_local.py)

**Context**: The local IRIS client has temporary system prompt restrictions to prevent hallucinations while MCP tools are not integrated.

**Current Limitations** (in `IrisConfig.system_prompt`):
```
- NEVER use placeholder text like [Event Name], [Time], [Location], etc.
- NEVER pretend you can access real-time data, calendars, or external systems
- You DON'T have access to: fleet data, wallet balances, real-time prices, or game APIs
- If asked about these, explain you're in local testing mode without those integrations
```

**When to Remove**: After integrating:
1. MCP tools for fleet status, wallet balance, transaction history
2. CITADEL REST API wrappers for real-time game data
3. Price/market data endpoints

**Action Items**:
- [ ] Track MCP tool integration progress (ARCH-001)
- [ ] Update system prompt when tools are available
- [ ] Test that IRIS correctly uses tools instead of hallucinating
- [ ] Consider keeping some anti-hallucination rules even with tools (for edge cases)

**File Location**: `packages/voice-backend/iris_local.py:109-122`

---

### ISSUE-012: Ambient Audio Cues
**Severity**: üü¢ Low | **Created**: 2025-12-05 | **Status**: Idea
**Component**: web-app, voice

**Idea**: Subtle audio cues (chime, typing sound) as alternative to verbal acknowledgment during cloud model latency gaps.

**Rationale**: Less intrusive than "Got it", still fills cognitive gap.

**Action Items**:
- [ ] Design audio cue library
- [ ] Implement optional cue playback
- [ ] User preference toggle

---

### ISSUE-013: Narrator Model Warm-Up Protocol
**Severity**: üü¢ Low | **Created**: 2025-12-05
**Component**: narrator (Ollama)

**Problem**: First inference on cold model takes 3-14s (model loading). Subsequent calls are fast (130-300ms).

**Benchmark Results** (2025-12-05):
| Model | Cold Start | Warm Latency | Notes |
|-------|------------|--------------|-------|
| qwen2.5:7b | 3.0s | 98-215ms | Best balance |
| qwen2.5:14b | 6.6s | 130-360ms | Good quality |
| qwen2.5:32b | 14.1s | 200-860ms | Highest quality |
| mistral:7b | 3.8s | 131-229ms | No filtering (too talkative) |

**Solution**: Warm-up protocol on service start.

**Action Items**:
- [ ] Add narrator warm-up to startup sequence
- [ ] Send dummy inference on load
- [ ] Log warm-up timing

---

### ISSUE-014: Long Response Chunking for Voice
**Severity**: üü¢ Low | **Created**: 2025-12-05 | **Status**: Idea
**Component**: agent-core

**Problem**: Long responses (80+ seconds of audio) are exhausting to listen to. User can't skim.

**Idea**: Chunk responses with natural breakpoints:
```
IRIS: "Three things about your fleet status." [pause]
IRIS: "First, Alpha's docked at MRZ-1."       [pause]
IRIS: "Second, fuel's critical at 15%."       [pause]
```

User can interrupt after any chunk. Front-load critical info.

**Action Items**:
- [ ] Design response chunking logic
- [ ] Implement pause points in TTS
- [ ] Enable per-chunk interruption

---

## High Complexity Items (needs_decomposition)

These tasks have complexity >3.0 and should be broken down further before implementation.

### task_1_2_3: prepareTransaction tool
**Complexity**: 3.2 | **Epic**: MCP Server

**Issue**: High technical complexity (4.0) and risk (4.0) due to transaction construction and security requirements.

**Why flagged**:
- Creating unsigned Solana transactions requires deep blockchain knowledge
- Security-critical: mistakes could cause fund loss
- Must handle various transaction types (SPL transfers, SAGE operations)

**Decomposition** (updated 2025-12-02):

| Subtask | Description | Est. | Complexity | Status |
|---------|-------------|------|------------|--------|
| subtask_1_2_3_1 | **Spike**: Research Solana transaction structure | 0.5d | 1.5 | üî¥ |
| subtask_1_2_3_2 | Implement basic SOL transfer builder | 1d | 2.0 | üî¥ |
| subtask_1_2_3_3 | Add SPL token transfer support | 1d | 2.2 | üî¥ |
| subtask_1_2_3_4 | Add SAGE operation templates | 1d | 2.5 | ‚è∏Ô∏è DEFERRED |
| subtask_1_2_3_5 | Transaction validation & security checks | 0.5d | 2.5 | üî¥ |

**MVP Recommendation**: ‚è∏Ô∏è **DEFER ENTIRE TASK** - Voice assistant MVP doesn't need transaction preparation. Users check balances/status via voice, execute transactions via Star Atlas UI. Revisit post-MVP if user demand exists.

**Status**: ‚è∏Ô∏è Deferred (MVP scope cut)

---

### task_1_3_3: subscribeToFleetUpdates WebSocket
**Complexity**: 3.5 | **Epic**: MCP Server

**Issue**: High technical complexity (4.0), dependencies (4.0), and uncertainty (4.0) due to WebSocket state management and Solana account subscriptions.

**Why flagged**:
- Solana account subscriptions are complex (RPC WebSocket)
- State management across reconnections
- Must handle rate limiting and backpressure

**Decomposition** (updated 2025-12-02):

| Subtask | Description | Est. | Complexity | Status |
|---------|-------------|------|------------|--------|
| subtask_1_3_3_1 | Basic Solana WebSocket connection handler | 1d | 2.0 | üî¥ |
| subtask_1_3_3_2 | Account subscription management (subscribe/unsubscribe) | 1d | 2.5 | üî¥ |
| subtask_1_3_3_3 | Reconnection and state recovery | 0.5d | 2.8 | üî¥ |
| subtask_1_3_3_4 | Rate limiting and backpressure handling | 0.5d | 2.3 | üî¥ |
| subtask_1_3_3_5 | Fleet update event transformation (Solana ‚Üí IRIS format) | 0.5d | 2.0 | üî¥ |

**MVP Recommendation**: ‚è∏Ô∏è **DEFER - Use polling instead.** Implement `getFleetStatus` on 30-60s interval for MVP. Provides "near real-time" with far less complexity. WebSocket subscriptions are a post-MVP optimization when users demand faster updates.

**Status**: ‚è∏Ô∏è Deferred (MVP scope cut - use polling)

---

### task_4_3_3: Latency monitoring and optimization
**Complexity**: 3.5 | **Epic**: Voice Service

**Issue**: High technical complexity (4.0) and uncertainty (4.0) due to real-time performance requirements.

**Why flagged**:
- <500ms round-trip is aggressive target
- Multiple components in chain (WebRTC ‚Üí Chatterbox ‚Üí Agent ‚Üí Chatterbox ‚Üí WebRTC)
- Optimization requires profiling across distributed system

**Decomposition** (updated 2025-12-02):

| Subtask | Description | Est. | Complexity | Status | MVP? |
|---------|-------------|------|------------|--------|------|
| subtask_4_3_3_1 | Add timing instrumentation to voice pipeline | 0.5d | 2.0 | üî¥ | ‚úÖ Yes |
| subtask_4_3_3_2 | Create latency logging/dashboard | 0.5d | 1.8 | üî¥ | ‚úÖ Yes |
| subtask_4_3_3_3 | Profile and identify bottlenecks | 0.5d | 2.5 | üî¥ | Post-MVP |
| subtask_4_3_3_4 | Implement streaming optimizations (overlap STT/Agent/TTS) | 1d | 3.0 | üî¥ | Post-MVP |
| subtask_4_3_3_5 | Add latency alerts and text-only fallback trigger | 0.5d | 2.2 | üî¥ | ‚úÖ Yes |

**MVP Recommendation**: Implement 4_3_3_1, 4_3_3_2, and 4_3_3_5 only. Get baseline measurements first, then optimize post-MVP. Text-only should be the default fallback if voice latency exceeds threshold.

**Status**: üü° Partially in scope (3 of 5 subtasks for MVP)

---

## Needs Spike Investigation

These items have high uncertainty (‚â•4) and need research before implementation.

### SPIKE: SAGE SDK Fleet Enumeration
**Feature**: task_1_3_1 enhancement (getFleetStatus full implementation)
**Uncertainty**: 4.0

**Current State**:
MVP implementation verifies player profile existence. Full fleet data (ship counts, states, resources) requires proper SAGE SDK integration with Anchor IDL parsing.

**Questions to answer**:
1. How to load and use @staratlas/sage IDL with Anchor?
2. What's the correct account structure for Fleet enumeration?
3. How to filter Fleet accounts by player profile efficiently?
4. What's the data size/RPC cost for fetching all fleet data?

**Suggested spike structure**:
```yaml
BLUEPRINT-spike-sage-sdk-integration-20251203.yaml

deliverables:
  - Research report: SAGE SDK account structures
  - POC: Fetch one fleet's full state
  - Decision: IDL loading strategy
  - Documentation: Fleet data parsing guide

duration: 2-3 days
```

**Status**: üîç Needs spike before full implementation

---

### SPIKE: Chatterbox Integration
**Feature**: feature_4_1 (Chatterbox Installation & Configuration)
**Uncertainty**: 4.0

**Questions to answer**:
1. What Docker image/setup works best on DO VPS?
2. What are the RAM/CPU requirements?
3. How to configure STT/TTS endpoints?
4. What models are available and their latency characteristics?
5. How to handle concurrent audio streams?

**Suggested spike structure**:
```yaml
BLUEPRINT-spike-chatterbox-integration-20251202.yaml

deliverables:
  - Research report: Chatterbox deployment options
  - POC: Basic STT/TTS working on VPS
  - Decision: Model selection for latency vs quality
  - Documentation: Setup steps for team

duration: 3-5 days
```

**Status**: üîç Needs spike before implementation

---

## Risk Register

### High Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| Voice latency >500ms | High | Medium | Early Chatterbox spike + fallback to text-only |
| VPS RAM insufficient | High | Low | Load test early, upgrade path ($24/month for 8GB) |
| Solana RPC rate limiting | High | Medium | Aggressive caching (5-min TTL), paid RPC fallback |

### Medium Risks

| Risk | Impact | Probability | Mitigation |
|------|--------|-------------|------------|
| WebSocket stability on mobile | Medium | Medium | Auto-reconnect, polling fallback |
| SQLite concurrent writes | Medium | Low | WAL mode, connection pool, PostgreSQL migration path |
| Star Atlas API changes | Medium | Low | Version lock SAGE SDK, monitor changelog |

---

## Dependencies

### Blocking Dependencies

| Dependency | Blocks | Notes |
|------------|--------|-------|
| Epic 1 (MCP Server) | Epic 3 (Agent Core) | Agent needs MCP tools |
| Epic 2 (Memory Service) | Epic 3 (Agent Core) | Agent needs conversation context |
| Feature 3.4 (Agent API) | Feature 4.4 (Voice API) | Voice needs agent endpoints |

### External Dependencies

| Service | Criticality | Fallback |
|---------|-------------|----------|
| Solana blockchain | High | Cached data, offline mode |
| Star Atlas SAGE API | High | Cached fleet state |
| CoinGecko API | Medium | Stale prices acceptable |
| Chatterbox | High | Text-only fallback |

---

## Assumptions

1. ‚úÖ Digital Ocean VPS has 640MB+ RAM available (verified 2025-12-01)
2. ‚ö†Ô∏è Chatterbox can run alongside other services (needs spike)
3. ‚ö†Ô∏è Star Atlas SAGE API will remain stable (monitor)
4. ‚úÖ SQLite performance adequate for <500 users
5. ‚úÖ WebRTC supported in target browsers

---

## CITADEL Integration (Epic 8)

### task_8_10_2: E2E tests for voice queries
**Complexity**: 3.0 | **Epic**: CITADEL Integration

**Issue**: High technical complexity (4.0) and dependencies (4.0) - requires both IRIS and CITADEL running.

**Why flagged**:
- Playwright tests need to simulate voice input
- Requires CITADEL API running or mocked
- Complex assertion chain: voice ‚Üí agent ‚Üí CITADEL API ‚Üí response

**Suggested breakdown**:
1. Mock CITADEL API responses
2. Test voice input simulation separately
3. Integration test voice ‚Üí agent flow
4. Full E2E with mock CITADEL

**Status**: üî¥ Not started

---

### CITADEL ‚Üî IRIS Coordination Notes

**Integration Contract** (defined in blueprint):
```
CITADEL Provides:
‚îú‚îÄ‚îÄ GET /api/dashboard/prices (30s cache)
‚îú‚îÄ‚îÄ GET /api/dashboard/fleets/:fleetId (60s cache)
‚îú‚îÄ‚îÄ GET /api/recommendations/mining (5min cache)
‚îú‚îÄ‚îÄ GET /api/recommendations/transport/:from/:to (5min cache)
‚îú‚îÄ‚îÄ WebSocket ws://citadel/ws/prices
‚îî‚îÄ‚îÄ MCP Tools: citadel_calculate_*, citadel_get_*

IRIS Provides:
‚îú‚îÄ‚îÄ Voice interface
‚îú‚îÄ‚îÄ User preferences from knowledge graph
‚îî‚îÄ‚îÄ Session context
```

**Parallel Development Strategy**:
- IRIS can start immediately with mock APIs (task_8_1_3)
- CITADEL Decision API priority: /api/dashboard/prices first
- Integration testing blocked until both systems have staging

**Risks**:
1. **CITADEL APIs not ready** ‚Üí Mitigation: Mock responses unblock IRIS
2. **MCP incompatibility** ‚Üí Mitigation: HTTP API fallback
3. **Voice latency budget** ‚Üí Mitigation: Aggressive caching (30s-5min TTLs)

---

## Aspirational Features (Distribution & Packaging)

### ASP-004: Linux Distribution via Self-Hosted APT Repository
**Type**: Packaging | **Priority**: Future
**Origin**: Riff session 2025-12-05

**Vision**: Enable `apt install iris-local` for Ubuntu users via self-hosted repository.

**Why APT (not Snap)**:
- No sandbox restrictions (Snap blocks some audio/hardware access)
- Standard Ubuntu UX (`apt install/remove/upgrade`)
- Auto-updates via normal apt mechanisms
- Full system access for GPU, audio devices

**Implementation (Brave browser pattern)**:
```bash
# One-time setup (user runs once)
curl -fsSL https://iris.arcforge.dev/setup-repo.sh | sudo bash

# Then forever after:
sudo apt install iris-local    # Install
sudo apt upgrade iris-local    # Update
sudo apt remove iris-local     # Uninstall
```

**Components Needed**:
1. `.deb` package structure
2. GPG signing key for package authenticity
3. Package hosting (GitHub Releases or S3)
4. Setup script that adds repo + key
5. GitHub Action to auto-build .deb on release

**Package Contents (~2-3GB fully bundled)**:
- Python + all pip dependencies
- Whisper STT model (~150MB)
- Kokoro TTS model (~350MB)
- Silero VAD model (~10MB)
- ffmpeg, PortAudio dependencies
- Desktop launcher + icon

**User Still Needs (external)**:
- Ollama + LLM model (~4GB for qwen2.5:7b)
- NVIDIA drivers (system-specific)

**Alternative: Install Script**:
Simpler approach for initial distribution:
```bash
curl -fsSL https://iris.arcforge.dev/install.sh | bash
```
Downloads ~3GB, installs everything, creates desktop entry.

**Trade-offs**:

| Approach | Pros | Cons |
|----------|------|------|
| **APT repo** | Standard UX, auto-updates | Complex setup, hosting costs |
| **Install script** | Simple, quick to implement | No auto-update, less "official" |
| **Snap** | Bundles everything | Sandbox issues, large size |
| **AppImage** | Single portable file | No auto-update, large |

**Recommendation**: Start with install script for early users, graduate to APT repo when user base grows.

**Status**: üîÆ Aspirational (post-MVP, post initial user validation)

---

### ASP-005: Product Flavors (Star Atlas vs Generic Assistant)
**Type**: Product | **Priority**: Future
**Origin**: Riff session 2025-12-05

**Vision**: Offer IRIS as both a Star Atlas-specific assistant AND a generic voice assistant with tools.

**Rationale**:
- Current implementation is tightly coupled to Star Atlas persona/tools
- Many users might want the voice + tools architecture without game specifics
- Broader market = more users = more feedback

**Proposed Flavors**:

| Flavor | Persona | Tools | Target User |
|--------|---------|-------|-------------|
| **IRIS (Star Atlas)** | "Guy in the chair" | Fleet, wallet, SAGE tools | Star Atlas players |
| **IRIS (Generic)** | Helpful assistant | File, web, shell, calendar | Developers, power users |
| **IRIS (Custom)** | User-defined | Plugin architecture | Builders |

**Implementation Approach**:
```
iris-core/           # Shared: audio, VAD, STT, TTS, LLM integration
iris-staratlas/      # Star Atlas persona + tools
iris-assistant/      # Generic assistant persona + tools
iris-plugins/        # Plugin architecture for custom tools
```

**Configuration**:
```yaml
# ~/.config/iris/config.yaml
flavor: staratlas  # or: generic, custom
persona:
  name: "IRIS"
  style: "guy_in_chair"  # or: professional, casual, custom
tools:
  enabled:
    - fleet_status
    - wallet_balance
  disabled:
    - file_operations
```

**Key Decisions Needed**:
1. How much Star Atlas code is entangled with core?
2. Plugin architecture for tools (MCP-style?)
3. Persona customization depth (just name? or full system prompt?)
4. Packaging: one binary with flags, or separate packages?

**Prerequisites**:
- Stable core architecture (current Python GUI)
- Clear separation of concerns in codebase
- User feedback on desired generic tools

**Status**: üîÆ Aspirational (post-MVP, requires architecture refactor)

---

## Aspirational Features (Native Client)

### ASP-003: Native Compiled Client (Rust/C++)
**Type**: Architecture | **Priority**: Future
**Origin**: Riff session 2025-12-05

**Vision**: Single compiled binary for maximum performance and game engine integration.

**Why Compiled Native**:
- **Unreal Engine integration**: Star Atlas uses Unreal (C++), native client could be an in-game overlay
- **Minimal latency**: No Python interpreter, no web stack overhead
- **Single binary distribution**: Easy install for users with compatible hardware
- **Game modding**: Plugin architecture for various games

**Target Architecture**:
```
Single Rust/C++ Binary:
‚îú‚îÄ‚îÄ Audio I/O (cpal/portaudio)
‚îú‚îÄ‚îÄ VAD (Silero ONNX or custom)
‚îú‚îÄ‚îÄ STT (whisper.cpp)
‚îú‚îÄ‚îÄ LLM (llama.cpp or Ollama client)
‚îú‚îÄ‚îÄ TTS (??? - need Rust TTS solution)
‚îî‚îÄ‚îÄ GUI (egui/imgui)
```

**Key Challenges**:
1. **TTS**: Kokoro is Python-only. Options:
   - whisper.cpp style port of Kokoro
   - Piper TTS (C++ native, ONNX models)
   - espeak-ng (lower quality but native)
   - Keep TTS as separate Python service (hybrid)

2. **Model Loading**: GPU memory management for multiple models

3. **Build Complexity**: CUDA/ROCm support across platforms

**Phased Approach**:
```
Phase 1 (Current): Python native GUI - validate architecture
Phase 2: Rust audio + VAD wrapper around Python backend
Phase 3: Replace STT with whisper.cpp
Phase 4: Replace LLM with llama.cpp (or keep Ollama)
Phase 5: Replace TTS (biggest unknown)
Phase 6: Full native binary
```

**Unreal Engine Integration Path**:
- Phase 1-2: External process, IPC via localhost socket
- Phase 3+: Could become Unreal plugin (C++ compatible)
- Audio routing: Capture game audio context, output to game audio system

**Alternative: Hybrid Architecture**:
```
Rust Frontend (audio + GUI):
‚îú‚îÄ‚îÄ Audio capture/playback (native)
‚îú‚îÄ‚îÄ VAD (native ONNX)
‚îú‚îÄ‚îÄ GUI (egui)
‚îî‚îÄ‚îÄ IPC to Python backend

Python Backend (unchanged):
‚îú‚îÄ‚îÄ STT (faster-whisper)
‚îú‚îÄ‚îÄ LLM (Ollama)
‚îî‚îÄ‚îÄ TTS (Kokoro)
```

This hybrid gets 80% of the benefit with 20% of the effort.

**Prerequisites**:
- Python native GUI working and validated
- Performance benchmarks establishing baseline
- TTS solution research spike

**Status**: üîÆ Aspirational (post Python GUI validation)

---

## Aspirational Features (from CITADEL)

> **Note**: These ML/RL features were originally scoped for CITADEL but moved to IRIS since IRIS is the "intelligent assistant" layer. CITADEL provides reliable data; IRIS provides intelligence.

### ASP-001: Price Prediction Model
**Type**: Machine Learning | **Priority**: Future
**Origin**: CITADEL Epic 6 (removed from scope)

**Concept**: Time-series forecasting (ARIMA, LSTM, Prophet) for Star Atlas resource prices.

**Why IRIS (not CITADEL)**:
- Predictions are recommendations, not raw data
- IRIS provides intelligent insights; CITADEL provides facts
- Can leverage CITADEL's historical data API

**Prerequisites**:
- CITADEL Milestone 2 (TimescaleDB historical data)
- Sufficient price history (~90 days minimum)
- Spike to assess if market has predictable patterns

**Rough scope**:
1. Feature engineering from CITADEL historical API
2. Model training pipeline (Python/scikit-learn)
3. Prediction endpoint for IRIS agent to consume
4. Confidence scoring and fallback to heuristics

**Status**: üîÆ Aspirational (post-MVP)

---

### ASP-002: Automated Strategy Optimizer (RL Agent)
**Type**: Reinforcement Learning | **Priority**: Future
**Origin**: CITADEL Epic 6 (removed from scope)

**Concept**: RL agent that learns optimal fleet allocation and operation strategies.

**Why IRIS (not CITADEL)**:
- Strategy is intelligence, not automation
- IRIS is the "brain"; CITADEL is the "hands"
- Agent can suggest to user; user decides to execute

**Prerequisites**:
- ASP-001 working (price predictions inform rewards)
- CITADEL profitability calculators stable
- Environment simulator for safe training (no real fleets)

**Rough scope**:
1. Design state/action/reward spaces
2. Build simulator using CITADEL APIs
3. Train with Stable-Baselines3 (DQN or PPO)
4. Shadow mode: suggest but don't execute
5. User approval gate before any real actions

**Status**: üîÆ Aspirational (post-MVP, post ASP-001)

---

## Architectural Issues

### ARCH-003: Voice Pipeline Architecture Overhead (12 Network Hops)
**Severity**: ‚úÖ Resolved | **Created**: 2025-12-03 | **Resolved**: 2025-12-03

**Issue**: The voice pipeline had excessive architecture overhead - 12 network crossings, 11 serialization boundaries, 4 language boundaries, and ~76% encoding overhead from base64.

**Previous Architecture (problematic)**:
```
Browser ‚Üí WS ‚Üí Node.js(voice-service) ‚Üí HTTP ‚Üí Python(STT) ‚Üí HTTP ‚Üí
Node.js ‚Üí WS ‚Üí Browser ‚Üí HTTP ‚Üí Node.js(agent) ‚Üí HTTP/2 ‚Üí Claude ‚Üí
SSE ‚Üí Node.js ‚Üí SSE ‚Üí Browser ‚Üí WS ‚Üí Node.js ‚Üí HTTP ‚Üí Python(TTS) ‚Üí
HTTP ‚Üí Node.js ‚Üí WS(base64) ‚Üí Browser
```

**New Architecture (implemented)**:
```
Browser ‚ÜêWebSocket(binary)‚Üí Python voice-backend ‚ÜêHTTP/2‚Üí Claude API
```
- **3 network hops** (vs 12)
- **Binary WebSocket** (2-byte header + raw PCM)
- **Node.js voice-service deprecated**

**Implementation Complete**:

| Phase | Description | Savings | Status |
|-------|-------------|---------|--------|
| **Phase 1**: Kill Node.js voice-service | Direct Browser ‚Üí Python WebSocket | ~40-80ms | ‚úÖ |
| **Phase 2**: Binary WebSocket frames | Raw PCM instead of base64 | ~30-50ms | ‚úÖ |
| **Phase 3**: Streaming STT | Process during recording | ~50-100ms | ‚úÖ |
| **Phase 4**: GPU STT | faster-whisper on CUDA | ~100-200ms | ‚úÖ |
| **Phase 5**: Plan Rust gateway | Future optimization | TBD | üìù Documented |

**Results**:
- STT: 181ms (GPU CUDA)
- TTS: 520ms (GPU CUDA)
- Fast-layer acknowledgments: 3-12ms (pattern-based)
- Total time to first audio: ~700ms

**Status**: ‚úÖ Complete - all phases implemented

---

### ARCH-002: Voice Latency Exceeds Natural Conversation Threshold
**Severity**: ‚úÖ Resolved | **Created**: 2025-12-02 | **Resolved**: 2025-12-03

**Issue**: End-to-end voice latency was ~6.2 seconds, far exceeding the <500ms target for natural conversation.

**Empirical Measurements** (2025-12-02, hybrid STT=CPU/TTS=CUDA):
```
Stage                           Mean        Min        Max        Std
-----------------------------------------------------------------
STT (faster-whisper CPU)      266.4ms     258.1ms     270.6ms       5.9ms
Claude API (first token)     1975.2ms    1697.1ms    2450.1ms     337.4ms
Claude API (total)           5030.5ms    4163.6ms    6589.4ms    1104.6ms
TTS (Chatterbox GPU)          896.3ms     888.3ms     905.2ms       6.9ms
-----------------------------------------------------------------
TOTAL E2E                    6193.2ms    5329.3ms    7748.4ms    1101.9ms
```

**Latency Breakdown**:
```
  User stops speaking
       ‚Üì
  STT Processing:      266.4ms (  4.3%)
       ‚Üì
  Claude API:         5030.5ms ( 81.2%)  ‚Üê BOTTLENECK
       ‚Üì
  TTS Synthesis:       896.3ms ( 14.5%)
       ‚Üì
  First audio plays

  TOTAL:              6193.2ms
```

**Root Cause Analysis**:
1. **Claude API is the bottleneck** (81.2% of total latency)
2. **Current architecture waits for full response** before starting TTS
   - Location: `packages/web-app/src/components/Chat.tsx:118-125`
   - TTS triggers on `type: "done"` event (full completion)
   - Streaming tokens arrive but aren't used until stream ends

**Optimization Options** (prioritized by impact):

| Option | Effort | Impact | Description |
|--------|--------|--------|-------------|
| **1. Sentence-level TTS streaming** | High | ~50% reduction | Start TTS on first sentence, don't wait for full response |
| **2. Use Claude Haiku for voice** | Low | ~40% reduction | Faster model for voice queries (Sonnet for complex tasks) |
| **3. Speculative first response** | Medium | ~30% reduction | Pre-generate greeting while processing |
| **4. Parallel STT during recording** | Low | ~5% reduction | Start transcription during silence gaps |

**Implementation Plan**:
```
Phase 1 (Quick wins):
- [ ] Switch to Claude Haiku for voice interactions
- [ ] Measure new latency with benchmark

Phase 2 (Streaming optimization):
- [x] Implemented fast-layer acknowledgments (pattern-based, 3-12ms)
- [x] Direct Anthropic SDK for Haiku (bypasses Agent SDK overhead)
- [ ] Sentence boundary TTS (deferred - current latency acceptable)
```

**Benchmark Scripts**:
- `packages/voice-backend/test_e2e_latency.py` (original)
- `packages/voice-backend/test_e2e_latency_v2.py` (with styles)
- `packages/voice-backend/test_e2e_latency_v3.py` (pattern vs Haiku validation)

**Resolution** (2025-12-03):
- Fast-layer pattern matching: 3-12ms acknowledgments
- GPU acceleration: STT 181ms, TTS 520ms
- Total time to first audio: ~700ms (vs 6.2s before)
- Remaining gap to 500ms target is acceptable for MVP

**Status**: ‚úÖ Resolved - latency reduced from 6.2s to ~700ms

---

### ARCH-001: IRIS/CITADEL Separation of Concerns
**Severity**: üî¥ Critical | **Created**: 2025-12-02 | **Updated**: 2025-12-02

**Issue**: IRIS MCP server currently contains blockchain/game data tools that belong in CITADEL.

**Current State**:
The `mcp-staratlas-server` package contains tools that should be CITADEL's responsibility:
- `getWalletBalance` - Solana RPC query
- `getTransactionHistory` - Solana RPC query
- `getFleetStatus` - SAGE SDK integration
- `predictFuelDepletion` - Game calculation with on-chain formulas

**Architecture Decision (2025-12-02)**:

| Layer | CITADEL | IRIS |
|-------|---------|------|
| **REST API** | ‚úÖ Provides | Consumes |
| **WebSocket** | ‚úÖ Provides | Consumes |
| **MCP Tools** | ‚ùå None | ‚úÖ Wraps REST |
| **Blockchain data** | ‚úÖ | ‚ùå |
| **Game calculations** | ‚úÖ | ‚ùå |
| **Voice interface** | ‚ùå | ‚úÖ |
| **User memory** | ‚ùå | ‚úÖ |

**Key Decision: MCP tools live in IRIS, not CITADEL**

Rationale:
- Separation of concerns: Citadel = data/execution, IRIS = intelligence
- Tool descriptions should be tailored to IRIS's persona
- IRIS can compose Citadel data with other sources (user memory, etc.)
- REST is the universal contract; MCP is Claude-specific
- Keeps Citadel simpler

**Implementation Pattern**:
```
IRIS MCP Tool ‚Üí wraps ‚Üí CITADEL REST API ‚Üí queries ‚Üí Solana/SAGE
```

**Example**:
```typescript
// IRIS MCP tool (in iris/packages/mcp-staratlas-server)
const iris_get_fleet_status = async (fleetId: string) => {
  const response = await fetch(`${CITADEL_API}/api/dashboard/fleets/${fleetId}`);
  return response.json();
};
```

**Detailed Design**: See `citadel/docs/IRIS-INTEGRATION.md`

**Action Items**:
1. [x] ~~Create CITADEL repository with MCP server~~ ‚Üí MCP stays in IRIS
2. [ ] CITADEL: Implement REST API endpoints for blockchain/game data (Epic 2-3)
3. [ ] CITADEL: Implement "Galactic Data Service" pattern for cached game data
4. [ ] IRIS: Update MCP tools to wrap CITADEL REST API (Epic 8) - **BLOCKED on Citadel API**
5. [ ] IRIS: Remove direct Solana RPC calls from MCP tools

**Dependency Chain**:
```
Citadel REST API (Epic 2-3) ‚Üí IRIS MCP wrappers (Epic 8) ‚Üí Voice agent queries
```

**Why This Matters**:
- Game formulas are on-chain (SAGE program) - hardcoding them in IRIS will break when Star Atlas updates
- CITADEL should own the "source of truth" for all blockchain/game data
- IRIS should be a thin voice layer + user context, not a data service
- Prevents duplicate RPC costs and inconsistent caching strategies

**Status**: üü° In Progress - Citadel REST API under development, IRIS MCP blocked until complete

---

## Action Items

1. [x] **ARCH-001**: Architecture decision made - MCP tools stay in IRIS, wrap Citadel REST (2025-12-02)
2. [ ] **ARCH-001**: Wait for Citadel REST API (Epic 2-3) before updating IRIS MCP tools (BLOCKED)
3. [ ] **Create spike blueprint** for Chatterbox integration
4. [x] **Decompose task_1_2_3** (prepareTransaction) - DONE, DEFERRED from MVP (2025-12-02)
5. [x] **Decompose task_1_3_3** (subscribeToFleetUpdates) - DONE, DEFERRED from MVP (2025-12-02)
6. [x] **Decompose task_4_3_3** (latency monitoring) - DONE, partial MVP scope (2025-12-02)
7. [ ] **Set up monitoring** for external API availability
8. [ ] **Coordinate with CITADEL** on API timeline and contract validation
9. [ ] **Create mock API** responses for CITADEL endpoints (task_8_1_3)
10. [ ] **Spike ASP-001** after CITADEL historical data available (assess prediction feasibility)

## MVP Scope Decisions (2025-12-02)

| Decision | Rationale |
|----------|-----------|
| **DEFER prepareTransaction** | Voice assistant checks status; users execute via Star Atlas UI |
| **DEFER WebSocket subscriptions** | Use polling (30-60s) for MVP; near real-time is sufficient |
| **SKIP traditional CI/CD** | Main-only workflow; deploy via `docker-compose` on VPS |
| **Text-only fallback default** | Voice is aspirational for MVP; must work without it |
| **MCP tools stay in IRIS** | IRIS wraps Citadel REST API; keeps separation clean (ARCH-001) |
| **BLOCKED: MCP wrapper updates** | Wait for Citadel REST API (Epic 2-3) before refactoring MCP tools |

---

**Note**: Update this file as issues are resolved or new ones discovered.
